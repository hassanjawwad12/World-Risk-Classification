# -*- coding: utf-8 -*-
"""disaster-classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eEP4ntBUeEM-hR225OwUXb32D_3WTIzT

**Introduction to the Problem**

Disasters, both natural and man-made, pose significant threats to communities and ecosystems worldwide. The impact of disasters can be devastating, resulting in loss of life, property damage, and disruption of essential services. In recent years, there has been a growing recognition of the importance of understanding and managing disaster risk to mitigate the adverse effects.

For this project, we focused on building a classification model to assess disaster risk based on the World Risk Index (WRI) data. The WRI incorporates various factors such as exposure, vulnerability, and susceptibility to determine the level of risk faced by different regions. By accurately classifying the severity of disaster risk, we aim to provide valuable insights for disaster preparedness and response efforts.

The "WRI Category" in the context of your dataset refers to the Weather-related Disaster Risk Index (WRI) category. This index is a measure used to assess the potential for weather-related disasters to occur in a particular location. The categories typically used for the WRI are:

1. **Low**: This category indicates a low risk of weather-related disasters. The weather conditions are unlikely to cause significant damage or disruption.

2. **Medium**: This category suggests a moderate risk of weather-related disasters. While not extremely severe, the potential for damage or disruption is likely to occur.

3. **High**: This category signifies a high risk of weather-related disasters. The weather conditions are likely to cause significant damage or disruption.

4. **Very High**: This category indicates a very high risk of weather-related disasters. The weather conditions are highly likely to cause significant damage or disruption.

The WRI category is derived from the WRI score, which is a numerical value that ranges from 0 to 100. The categories are often determined based on predefined thresholds, such as:

- **Low**: WRI score < 25
- **Medium**: WRI score 25-50
- **High**: WRI score 50-75
- **Very High**: WRI score > 75

The WRI category is used to help prioritize disaster risk management efforts and to inform public awareness and preparedness.
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import arviz as az
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
df=pd.read_csv('world_risk_index.csv')

print(df)

df.head()

df.tail()

df.describe()

df.info()

df.shape

"""**Preprocessing Steps**

In our preprocessing steps, we first collected the data from kaggle.
We handled missing values by replacing them with the mean of the respective features, ensuring that no information was lost during the preprocessing stage.

Additionally, we normalized the data to bring all features to a similar scale, preventing any biases during model training.

We also fixed the column names.

We replaced categorical values with numerical values, this known as label encoding. It's often done to convert categorical data into a format that can be used for modeling or analysis, as many machine learning algorithms require numerical input.



"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder

df.isnull().sum()

df.rename(columns={
    ' Lack of Adaptive Capacities':'Lack of Adaptive Capacities'
},inplace = True)
df.isnull().sum()

df['Lack of Adaptive Capacities'].fillna(df['Lack of Adaptive Capacities'].mean(),inplace=True)
df.isnull().sum()

df.dropna(inplace = True)

df.isnull().sum()

df.replace({'Exposure Category': {'Very High': 5}}, inplace = True)
df.replace({'Exposure Category': {'High': 4}}, inplace = True)
df.replace({'Exposure Category': {'Medium': 3}}, inplace = True)
df.replace({'Exposure Category': {'Low': 2}}, inplace = True)
df.replace({'Exposure Category': {'Very Low': 1}}, inplace = True)

df.replace({'Vulnerability Category': {'Very High': 5}}, inplace = True)
df.replace({'Vulnerability Category': {'High': 4}}, inplace = True)
df.replace({'Vulnerability Category': {'Medium': 3}}, inplace = True)
df.replace({'Vulnerability Category': {'Low': 2}}, inplace = True)
df.replace({'Vulnerability Category': {'Very Low': 1}}, inplace = True)

df.replace({'Susceptibility Category': {'Very High': 5}}, inplace = True)
df.replace({'Susceptibility Category': {'High': 4}}, inplace = True)
df.replace({'Susceptibility Category': {'Medium': 3}}, inplace = True)
df.replace({'Susceptibility Category': {'Low': 2}}, inplace = True)
df.replace({'Susceptibility Category': {'Very Low': 1}}, inplace = True)

df.replace({'WRI Category': {'Very High': 5}}, inplace = True)
df.replace({'WRI Category': {'High': 4}}, inplace = True)
df.replace({'WRI Category': {'Medium': 3}}, inplace = True)
df.replace({'WRI Category': {'Low': 2}}, inplace = True)
df.replace({'WRI Category': {'Very Low': 1}}, inplace = True)

df.head()

#VISUALIZATIONS
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.hist(df['WRI'], bins=20, color='skyblue', edgecolor='black')
plt.xlabel('WRI (Water Risk Index)')
plt.ylabel('Frequency')
plt.title('Histogram of WRI')
plt.grid(True)
plt.show()

import seaborn as sns

plt.figure(figsize=(10, 6))
sns.boxplot(x='Exposure Category', y='Exposure', data=df)
plt.xlabel('Exposure Category')
plt.ylabel('Exposure')
plt.title('Box plot of Exposure by Exposure Category')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(df['Vulnerability'], df['Susceptibility'], alpha=0.5)
plt.xlabel('Vulnerability')
plt.ylabel('Susceptibility')
plt.title('Scatter plot of Vulnerability vs. Susceptibility')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(df['Year'], df['Lack of Coping Capabilities'], marker='o', color='green')
plt.xlabel('Year')
plt.ylabel('Lack of Coping Capabilities')
plt.title('Lack of Coping Capabilities over Years')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(x='Vulnerability Category', y='Lack of Adaptive Capacities', data=df)
plt.xlabel('Vulnerability Category')
plt.ylabel('Lack of Adaptive Capacities')
plt.title('Bar chart of Lack of Adaptive Capacities by Vulnerability Category')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(8, 8))
df['WRI Category'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=140)
plt.title('WRI Category distribution')
plt.axis('equal')
plt.show()

plt.figure(figsize=(12, 8))
df.pivot_table(index='Year', columns='Region', values='WRI', aggfunc='mean').plot(kind='bar', stacked=True)
plt.xlabel('Year')
plt.ylabel('WRI')
plt.title('WRI by Region and Year')
plt.legend(title='Region')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
sns.violinplot(x='Susceptibility Category', y='Vulnerability', data=df)
plt.xlabel('Susceptibility Category')
plt.ylabel('Vulnerability')
plt.title('Violin plot of Vulnerability Category by Susceptibility Category')
plt.xticks(rotation=45)
plt.show()

import seaborn as sns

sns.pairplot(df[['WRI', 'Exposure', 'Vulnerability', 'Susceptibility']])
plt.show()

# Convert 'WRI Category' to numerical categories
label_encoder = LabelEncoder()
df['WRI Category'] = label_encoder.fit_transform(df['WRI Category'])

"""**Model Choosing**

For this project, we opted for a Random Forest classification model due to its ability to handle complex datasets with multiple features.

Random Forest is an ensemble learning method that combines multiple decision trees to make predictions. We chose this model because it can effectively capture non-linear relationships between the input features and the target variable.


"""

# Define features and target
features = df[['Exposure', 'Vulnerability', 'Susceptibility', 'Lack of Coping Capabilities', 'Lack of Adaptive Capacities']]
target = df['WRI Category']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Train the model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Evaluate the model
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

"""**Results and Conclusion**

Our analysis yielded promising results, with the Random Forest classification model achieving an accuracy of 95 percent in predicting disaster risk categories. By accurately classifying the severity of disaster risk based on the WRI data, our model can provide valuable insights for decision-makers and stakeholders involved in disaster preparedness and response efforts.

In conclusion, our project demonstrates the potential of machine learning models in assessing and managing disaster risk. By leveraging data-driven approaches, we can enhance our understanding of environmental hazards and take proactive measures to mitigate their impact on communities and ecosystems.






"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.utils.multiclass import unique_labels

# Assuming y_test and y_pred are the true and predicted labels, respectively
cm = confusion_matrix(y_test, y_pred)

# Create a confusion matrix plot
fig, ax = plt.subplots(figsize=(10, 10))
sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues', square=True,
            xticklabels=unique_labels(y_test), yticklabels=unique_labels(y_test))
ax.set_xlabel('Predicted')
ax.set_ylabel('True')
ax.set_title('Confusion Matrix')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Assuming model is your trained RandomForestClassifier
importances = model.feature_importances_
feature_names = features.columns

# Create a DataFrame of feature importances
df_importances = pd.DataFrame({'feature': feature_names, 'importance': importances})
df_importances = df_importances.sort_values('importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x='importance', y='feature', data=df_importances)
plt.title('Random Forest Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

df.columns

column_name = 'Region'
unique_entries = df[column_name].unique()

print(unique_entries)

# Define the state you want to test
state = 'India'

# Filter the dataset for the state of interest
state_df = df[df['Region'] == state]

# Calculate the mean or median values for each feature
exposure_value = state_df['Exposure'].mean()
vulnerability_value = state_df['Vulnerability'].mean()
susceptibility_value = state_df['Susceptibility'].mean()
coping_capabilities_value = state_df['Lack of Coping Capabilities'].mean()
adaptive_capacities_value = state_df['Lack of Adaptive Capacities'].mean()

# Create the state_data dictionary
state_data = {
    'Exposure': [exposure_value],
    'Vulnerability': [vulnerability_value],
    'Susceptibility': [susceptibility_value],
    'Lack of Coping Capabilities': [coping_capabilities_value],
    'Lack of Adaptive Capacities': [adaptive_capacities_value]
}

# Print the state_data dictionary
print(state_data)

state_df = pd.DataFrame(state_data)

# Make prediction for the particular state
state_prediction = model.predict(state_df)
state_prediction_proba = model.predict_proba(state_df)

# Convert numerical category back to original category using label encoder
predicted_category = label_encoder.inverse_transform(state_prediction)

# Print the prediction
print("Predicted WRI Category for the state:", predicted_category)

# Print the probability estimates for each category
print("Probability Estimates for Each Category:")
for i, prob in enumerate(state_prediction_proba[0]):
    category = label_encoder.inverse_transform([i])[0]
    print(f"{category}: {prob}")

from ipywidgets import interact, widgets

# Define a function to calculate state data and make predictions
def calculate_state_data(state):
    # Filter the dataset for the state of interest
    state_df = df[df['Region'] == state]

    # Calculate the mean values for each feature
    exposure_value = state_df['Exposure'].mean()
    vulnerability_value = state_df['Vulnerability'].mean()
    susceptibility_value = state_df['Susceptibility'].mean()
    coping_capabilities_value = state_df['Lack of Coping Capabilities'].mean()
    adaptive_capacities_value = state_df['Lack of Adaptive Capacities'].mean()

    # Create the state_data dictionary
    state_data = {
        'Exposure': [exposure_value],
        'Vulnerability': [vulnerability_value],
        'Susceptibility': [susceptibility_value],
        'Lack of Coping Capabilities': [coping_capabilities_value],
        'Lack of Adaptive Capacities': [adaptive_capacities_value]
    }

    # Print the state_data dictionary
    print("State Data:")
    print(state_data)

    # Convert state data to DataFrame
    state_df = pd.DataFrame(state_data)

    # Make prediction for the particular state
    state_prediction = model.predict(state_df)
    state_prediction_proba = model.predict_proba(state_df)

    # Convert numerical category back to original category using label encoder
    predicted_category = label_encoder.inverse_transform(state_prediction)

    # Print the prediction
    print("\nPredicted WRI Category for the state:", predicted_category[0])

    # Print the probability estimates for each category
    print("\nProbability Estimates for Each Category:")
    for i, prob in enumerate(state_prediction_proba[0]):
        category = label_encoder.inverse_transform([i])[0]
        print(f"{category}: {prob:.2f}")

# Create a text input widget for the state
state_input = widgets.Text(
    value='India',
    placeholder='Enter state name',
    description='State:',
    disabled=False
)

# Define an interaction to trigger the calculation based on user input
interact(calculate_state_data, state=state_input);